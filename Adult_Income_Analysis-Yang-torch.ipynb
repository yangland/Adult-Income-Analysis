{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "\n",
    "# Pytorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adult = pd.read_csv('adult.csv')\n",
    "\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'educational-num','marital-status', 'occupation', 'relationship', 'race', 'gender','capital-gain', 'capital-loss', 'hours-per-week', 'native-country','income']\n",
    "\n",
    "train = pd.read_csv('adult_data.txt', sep=\",\\s\", header=None, names = column_names, engine = 'python')\n",
    "test = pd.read_csv('adult_test.txt', sep=\",\\s\", header=None, names = column_names, engine = 'python')\n",
    "test['income'].replace(regex=True,inplace=True,to_replace=r'\\.',value=r'')\n",
    "\n",
    "\n",
    "adult = pd.concat([test,train])\n",
    "adult.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1.1. Columns and their types"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   age              48842 non-null  int64   \n",
      " 1   workclass        48842 non-null  category\n",
      " 2   fnlwgt           48842 non-null  int64   \n",
      " 3   education        48842 non-null  category\n",
      " 4   educational-num  48842 non-null  int64   \n",
      " 5   marital-status   48842 non-null  category\n",
      " 6   occupation       48842 non-null  category\n",
      " 7   relationship     48842 non-null  category\n",
      " 8   race             48842 non-null  category\n",
      " 9   gender           48842 non-null  category\n",
      " 10  capital-gain     48842 non-null  int64   \n",
      " 11  capital-loss     48842 non-null  int64   \n",
      " 12  hours-per-week   48842 non-null  int64   \n",
      " 13  native-country   48842 non-null  category\n",
      " 14  income           48842 non-null  category\n",
      "dtypes: category(9), int64(6)\n",
      "memory usage: 2.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Setting all the categorical columns to type category\n",
    "for col in set(adult.columns) - set(adult.describe().columns):\n",
    "    adult[col] = adult[col].astype('category')\n",
    "    \n",
    "printmd('## 1.1. Columns and their types')\n",
    "print(adult.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1.2. Data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 records\n",
    "printmd('## 1.2. Data')\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1.3. Summary Statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48842.000000</td>\n",
       "      <td>4.884200e+04</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.643585</td>\n",
       "      <td>1.896641e+05</td>\n",
       "      <td>10.078089</td>\n",
       "      <td>1079.067626</td>\n",
       "      <td>87.502314</td>\n",
       "      <td>40.422382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.710510</td>\n",
       "      <td>1.056040e+05</td>\n",
       "      <td>2.570973</td>\n",
       "      <td>7452.019058</td>\n",
       "      <td>403.004552</td>\n",
       "      <td>12.391444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.175505e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.781445e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.376420e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.490400e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  educational-num  capital-gain  \\\n",
       "count  48842.000000  4.884200e+04     48842.000000  48842.000000   \n",
       "mean      38.643585  1.896641e+05        10.078089   1079.067626   \n",
       "std       13.710510  1.056040e+05         2.570973   7452.019058   \n",
       "min       17.000000  1.228500e+04         1.000000      0.000000   \n",
       "25%       28.000000  1.175505e+05         9.000000      0.000000   \n",
       "50%       37.000000  1.781445e+05        10.000000      0.000000   \n",
       "75%       48.000000  2.376420e+05        12.000000      0.000000   \n",
       "max       90.000000  1.490400e+06        16.000000  99999.000000   \n",
       "\n",
       "       capital-loss  hours-per-week  \n",
       "count  48842.000000    48842.000000  \n",
       "mean      87.502314       40.422382  \n",
       "std      403.004552       12.391444  \n",
       "min        0.000000        1.000000  \n",
       "25%        0.000000       40.000000  \n",
       "50%        0.000000       40.000000  \n",
       "75%        0.000000       45.000000  \n",
       "max     4356.000000       99.000000  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printmd('## 1.3. Summary Statistics')\n",
    "\n",
    "adult.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1.4. Missing values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "workclass: 2799 records"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "occupation: 2809 records"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "native-country: 857 records"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd('## 1.4. Missing values')\n",
    "for i,j in zip(adult.columns,(adult.values.astype(str) == '?').sum(axis = 0)):\n",
    "    if j > 0:\n",
    "        printmd(str(i) + ': ' + str(j) + ' records')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Remove \"?\" value records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_pred = adult.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt   education  educational-num      marital-status  \\\n",
       "0   25    Private  226802        11th                7       Never-married   \n",
       "1   38    Private   89814     HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951  Assoc-acdm               12  Married-civ-spouse   \n",
       "\n",
       "          occupation relationship   race gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black   Male             0             0   \n",
       "1    Farming-fishing      Husband  White   Male             0             0   \n",
       "2    Protective-serv      Husband  White   Male             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_pred.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one hot encoding of the categorical columns in the data frame.\n",
    "def oneHotCatVars(df, df_cols):\n",
    "    \n",
    "    df_1 = adult_data = df.drop(columns = df_cols, axis = 1)\n",
    "    df_2 = pd.get_dummies(df[df_cols])\n",
    "    \n",
    "    return (pd.concat([df_1, df_2], axis=1, join='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep\n",
    "adult_data = adult_pred.drop(columns = ['income'])\n",
    "adult_label = adult_pred.income\n",
    "\n",
    "adult_label = adult_label.replace(\"<=50K\", 0)\n",
    "adult_label = adult_label.replace(\">50K\", 1)\n",
    "\n",
    "adult_cat_1hot = pd.get_dummies(adult_data.select_dtypes('category'))\n",
    "adult_non_cat = adult_data.select_dtypes(exclude = 'category')\n",
    "\n",
    "adult_data_1hot = pd.concat([adult_non_cat, adult_cat_1hot], axis=1, join='inner')\n",
    "adult_data_1hot = adult_data_1hot.replace(False, 0)\n",
    "adult_data_1hot = adult_data_1hot.replace(True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45222 entries, 0 to 48841\n",
      "Columns: 105 entries, age to native-country_Yugoslavia\n",
      "dtypes: int64(105)\n",
      "memory usage: 37.6 MB\n"
     ]
    }
   ],
   "source": [
    "adult_data_1hot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adult_data_1hot.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adult_data_1hot = torch.as_tensor(adult_data_1hot.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0, ..., 0, 1, 0, 0, 1]\n",
       "Length: 45222\n",
       "Categories (2, int64): [0, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adult_data_1hot = tf.convert_to_tensor(adult_data_1hot.values)\n",
    "# adult_label = tf.convert_to_tensor(adult_label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(adult_data_1hot, adult_label, test_size  = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33252    1\n",
       "37537    0\n",
       "39304    0\n",
       "18085    0\n",
       "2716     1\n",
       "        ..\n",
       "4149     0\n",
       "22021    0\n",
       "19833    1\n",
       "27650    0\n",
       "29117    0\n",
       "Name: income, Length: 33916, dtype: category\n",
       "Categories (2, int64): [0, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train, dtype= np.float32), np.array(y_train)\n",
    "X_valid, y_valid = np.array(X_test, dtype= np.float32), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPS05Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "# As you can see we define torch arrays - then we will put them into device\n",
    "train_dataset = TPS05Dataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "valid_dataset = TPS05Dataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TPS05Dataset at 0x2875d87bdf0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_FEATURES = len(X_train[0])\n",
    "NUM_CLASSES = 2\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders \n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPS05ClassificationModule(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(TPS05ClassificationModule, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_feature, 512)\n",
    "        self.layer_2 = nn.Linear(512, 256)\n",
    "        self.layer_3 = nn.Linear(256, 128)\n",
    "        self.layer_out = nn.Linear(128, num_class)\n",
    "        \n",
    "        torch.nn.init.xavier_normal_(self.layer_1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.layer_2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.layer_3.weight)\n",
    "        torch.nn.init.xavier_normal_(self.layer_out.weight)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(p=0.3)\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.batchnorm_1 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(256)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "       #x = self.batchnorm_1(x)\n",
    "        x = F.relu(x)   # Second one using torch.nn.functional\n",
    "        x = self.dropout_1(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "       #x = self.batchnorm_2(x)\n",
    "        x = self.softmax(x)\n",
    "        x = self.dropout_2(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        #x = self.batchnorm_3(x)\n",
    "        x = self.softmax(x)\n",
    "        x = self.dropout_2(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Day 3 (7.5.2021)- using Sequential\n",
    "# And ... what do you thing ... is it better? :)\n",
    "\n",
    "def linear_block(in_features, out_features, p_drop, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features),\n",
    "        #nn.BatchNorm1d(out_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p = p_drop)\n",
    "    )\n",
    "\n",
    "class TPS05ClassificationSeq(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(TPS05ClassificationSeq, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            # linear_block(num_feature, 100, 0.3),\n",
    "            # linear_block(100, 250, 0.3),\n",
    "            # linear_block(250, 128, 0.3),\n",
    "\n",
    "            linear_block(num_feature, 256, 0.3),\n",
    "            linear_block(256, 1024, 0.3),\n",
    "            linear_block(1024, 128, 0.3),\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, num_class)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Day 4 (8.5.2021) - using Dynamic Sequential \n",
    "class TPS05ClassificationDynSeq(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(TPS05ClassificationDynSeq, self).__init__()\n",
    "        \n",
    "        self.lin_sizes = [num_feature, 64, 32, 128]\n",
    "        self.b_norm = [0.3, 0.2, 0.2]\n",
    "        \n",
    "        lin_blocks = [linear_block(in_f, out_f, b_in) \n",
    "                      for in_f, out_f , b_in in zip(self.lin_sizes, self.lin_sizes[1:], self.b_norm)]\n",
    "        \n",
    "        self.linear = nn.Sequential(*lin_blocks)\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, num_class)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPS05ClassificationModule(\n",
      "  (layer_1): Linear(in_features=105, out_features=512, bias=True)\n",
      "  (layer_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (layer_3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (layer_out): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (dropout_1): Dropout(p=0.3, inplace=False)\n",
      "  (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "  (batchnorm_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "modelMod = TPS05ClassificationModule(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "# Then pushi it to device (CPU/GPU)\n",
    "modelMod.to(device)\n",
    "\n",
    "# model.eval() is switch off for some specific layers/parts of the model (Dropouts Layers, BatchNorm Layers etc.) \n",
    "modelMod.eval()\n",
    "\n",
    "# Whenever you want you can print model \n",
    "print(modelMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TPS05ClassificationSeq(\n",
       "  (linear): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=105, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model using Module \n",
    "modelSeq = TPS05ClassificationSeq(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "# Then pushi it to device (CPU/GPU)\n",
    "modelSeq.to(device)\n",
    "\n",
    "# model.eval() is switch off for some specific layers/parts of the model (Dropouts Layers, BatchNorm Layers etc.) \n",
    "modelSeq.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = modelSeq\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stat = {'train': [],\"validation\": []}\n",
    "loss_stat = {'train': [], \"validation\": [] }\n",
    "\n",
    "def acc_calc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "papermill": {
     "duration": 0.058869,
     "end_time": "2022-03-05T22:01:38.409946",
     "exception": false,
     "start_time": "2022-03-05T22:01:38.351077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is training and validation loop\n",
    "# for each epoch\n",
    "def train_nn():\n",
    "    for progress in tqdm(range(1, NUM_EPOCHS+1)):\n",
    "\n",
    "        train_epoch_loss = 0\n",
    "        train_epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # We loop over training dataset using batches (we use DataLoader to load data with batches)\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass ->>>>\n",
    "            y_train_pred = model(X_train_batch)\n",
    "\n",
    "            # Find Loss and backpropagation of gradients\n",
    "            train_loss = criterion(y_train_pred, y_train_batch)\n",
    "            train_acc = acc_calc(y_train_pred, y_train_batch)\n",
    "\n",
    "            # backward <------    \n",
    "            train_loss.backward()\n",
    "\n",
    "            # Update the parameters (weights and biases)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_epoch_loss += train_loss.item()\n",
    "            train_epoch_acc += train_acc.item()\n",
    "\n",
    "\n",
    "        #  Then we validate our model - concept is the same\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in valid_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = acc_calc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "        # end of validation loop\n",
    "        early_stopping_callback(val_epoch_loss/len(valid_loader))\n",
    "        if early_stopping_callback.stop_training:\n",
    "            print(f'Training stopped -> Early Stopping Callback : validation_loss: {val_epoch_loss/len(valid_loader)}')\n",
    "            break\n",
    "\n",
    "        loss_stat['train'].append(train_epoch_loss/len(train_loader))\n",
    "        loss_stat['validation'].append(val_epoch_loss/len(valid_loader))\n",
    "        accuracy_stat['train'].append(train_epoch_acc/len(train_loader))\n",
    "        accuracy_stat['validation'].append(val_epoch_acc/len(valid_loader))                           \n",
    "        \n",
    "        \n",
    "        # 2021.05.17 \n",
    "        # This is a part of NN optimization\n",
    "        clr = optimizer.param_groups[0]['lr']        \n",
    "        scheduler.step(val_epoch_acc/len(valid_loader))\n",
    "\n",
    "        print(f'Epoch { progress + 0:03}: Loss: [Train: {train_epoch_loss/len(train_loader):.5f} | Validation: {val_epoch_loss/len(valid_loader):.5f} ] Accuracy: [Train: {train_epoch_acc/len(train_loader):.3f} | Validation: {val_epoch_acc/len(valid_loader):.3f}] LR: {clr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "papermill": {
     "duration": 0.051825,
     "end_time": "2022-03-05T22:01:38.309527",
     "exception": false,
     "start_time": "2022-03-05T22:01:38.257702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's create simple Pytorch Callback \n",
    "\n",
    "class EarlyStoppingCallback:   \n",
    "    def __init__(self, min_delta = 0.1, patience = 5):\n",
    "        \n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best_epoch_score = 0\n",
    "        \n",
    "        self.attempt = 0\n",
    "        self.best_score = None\n",
    "        self.stop_training = False\n",
    "        \n",
    "        \n",
    "    def __call__(self, validation_loss):\n",
    "\n",
    "        self.epoch_score = validation_loss\n",
    "\n",
    "        if self.best_epoch_score == 0:\n",
    "            self.best_epoch_score = self.epoch_score\n",
    "        elif self.epoch_score > self.best_epoch_score - self.min_delta:\n",
    "            self.attempt += 1\n",
    "            print(f'Message from callback (Early Stopping) counter: {self.attempt}/{self.patience}')\n",
    "            if self.attempt >= self.patience:\n",
    "                self.stop_training = True\n",
    "        else:\n",
    "            self.best_epoch_score = self.epoch_score\n",
    "            self.attempt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d8f1cfde0c46c58643d37ecf8ac3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss: [Train: 14.04582 | Validation: 0.57395 ] Accuracy: [Train: 74.089 | Validation: 75.093] LR: 0.001\n",
      "Epoch 002: Loss: [Train: 0.57570 | Validation: 0.56076 ] Accuracy: [Train: 75.315 | Validation: 75.146] LR: 0.001\n",
      "Epoch 003: Loss: [Train: 0.56135 | Validation: 0.55620 ] Accuracy: [Train: 75.566 | Validation: 75.447] LR: 0.001\n",
      "Message from callback (Early Stopping) counter: 1/10\n",
      "Epoch 004: Loss: [Train: 0.56466 | Validation: 0.55709 ] Accuracy: [Train: 75.442 | Validation: 75.385] LR: 0.001\n",
      "Message from callback (Early Stopping) counter: 2/10\n",
      "Epoch 005: Loss: [Train: 0.57902 | Validation: 0.55895 ] Accuracy: [Train: 75.492 | Validation: 75.252] LR: 0.001\n",
      "Message from callback (Early Stopping) counter: 3/10\n",
      "Epoch 006: Loss: [Train: 0.55963 | Validation: 0.55969 ] Accuracy: [Train: 75.432 | Validation: 75.199] LR: 0.0001\n",
      "Message from callback (Early Stopping) counter: 4/10\n",
      "Epoch 007: Loss: [Train: 0.56010 | Validation: 0.56019 ] Accuracy: [Train: 75.443 | Validation: 75.164] LR: 0.0001\n",
      "Message from callback (Early Stopping) counter: 5/10\n",
      "Epoch 008: Loss: [Train: 0.55846 | Validation: 0.56043 ] Accuracy: [Train: 75.411 | Validation: 75.146] LR: 0.0001\n",
      "Message from callback (Early Stopping) counter: 6/10\n",
      "Epoch 009: Loss: [Train: 0.56279 | Validation: 0.56043 ] Accuracy: [Train: 75.375 | Validation: 75.146] LR: 0.0001\n",
      "Message from callback (Early Stopping) counter: 7/10\n",
      "Epoch 010: Loss: [Train: 0.55746 | Validation: 0.56043 ] Accuracy: [Train: 75.415 | Validation: 75.146] LR: 1e-05\n",
      "Message from callback (Early Stopping) counter: 8/10\n",
      "Epoch 011: Loss: [Train: 0.55823 | Validation: 0.56043 ] Accuracy: [Train: 75.406 | Validation: 75.146] LR: 1e-05\n",
      "Message from callback (Early Stopping) counter: 9/10\n",
      "Epoch 012: Loss: [Train: 0.55775 | Validation: 0.56031 ] Accuracy: [Train: 75.408 | Validation: 75.155] LR: 1e-05\n",
      "Message from callback (Early Stopping) counter: 10/10\n",
      "Training stopped -> Early Stopping Callback : validation_loss: 0.5603105914333363\n"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(0.001, 10)\n",
    "train_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old TF code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(actual, pred):\n",
    "    \n",
    "    confusion = pd.crosstab(actual, pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "    TP = confusion.loc['>50K','>50K']\n",
    "    TN = confusion.loc['<=50K','<=50K']\n",
    "    FP = confusion.loc['<=50K','>50K']\n",
    "    FN = confusion.loc['>50K','<=50K']\n",
    "\n",
    "    accuracy = ((TP+TN))/(TP+FN+FP+TN)\n",
    "    precision = (TP)/(TP+FP)\n",
    "    recall = (TP)/(TP+FN)\n",
    "    f_measure = (2*recall*precision)/(recall+precision)\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    error_rate = 1 - accuracy\n",
    "    \n",
    "    out = {}\n",
    "    out['accuracy'] =  accuracy\n",
    "    out['precision'] = precision\n",
    "    out['recall'] = recall\n",
    "    out['f_measure'] = f_measure\n",
    "    out['sensitivity'] = sensitivity\n",
    "    out['specificity'] = specificity\n",
    "    out['error_rate'] = error_rate\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yang - DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import MaxNorm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data, train_label, test_label\n",
    "train_data = tf.convert_to_tensor(train_data)\n",
    "train_label = tf.convert_to_tensor(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(33916, 105), dtype=int64, numpy=\n",
       "array([[    58,  82050,     10, ...,      1,      0,      0],\n",
       "       [    42,  24763,     13, ...,      1,      0,      0],\n",
       "       [    50,  57852,     16, ...,      1,      0,      0],\n",
       "       ...,\n",
       "       [    30, 329425,      9, ...,      1,      0,      0],\n",
       "       [    48, 273402,      4, ...,      1,      0,      0],\n",
       "       [    24, 265567,      7, ...,      1,      0,      0]], dtype=int64)>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5442 - accuracy: 0.7779\n",
      "Epoch 2/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5104 - accuracy: 0.7869\n",
      "Epoch 3/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5074 - accuracy: 0.7882\n",
      "Epoch 4/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5042 - accuracy: 0.7911\n",
      "Epoch 5/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5021 - accuracy: 0.7905\n",
      "Epoch 6/30\n",
      "3392/3392 [==============================] - 4s 1ms/step - loss: 0.5025 - accuracy: 0.7900\n",
      "Epoch 7/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5017 - accuracy: 0.7912\n",
      "Epoch 8/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5029 - accuracy: 0.7905\n",
      "Epoch 9/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5025 - accuracy: 0.7898\n",
      "Epoch 10/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5010 - accuracy: 0.7915\n",
      "Epoch 11/30\n",
      "3392/3392 [==============================] - 4s 1ms/step - loss: 0.5010 - accuracy: 0.7923\n",
      "Epoch 12/30\n",
      "3392/3392 [==============================] - 4s 1ms/step - loss: 0.5006 - accuracy: 0.7917\n",
      "Epoch 13/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5033 - accuracy: 0.7905\n",
      "Epoch 14/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5015 - accuracy: 0.7916\n",
      "Epoch 15/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5050 - accuracy: 0.7892\n",
      "Epoch 16/30\n",
      "3392/3392 [==============================] - 5s 2ms/step - loss: 0.5013 - accuracy: 0.7915\n",
      "Epoch 17/30\n",
      "3392/3392 [==============================] - 5s 2ms/step - loss: 0.5037 - accuracy: 0.7905\n",
      "Epoch 18/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.4996 - accuracy: 0.7926\n",
      "Epoch 19/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5003 - accuracy: 0.7922\n",
      "Epoch 20/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5011 - accuracy: 0.7926\n",
      "Epoch 21/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5013 - accuracy: 0.7906\n",
      "Epoch 22/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5038 - accuracy: 0.7902\n",
      "Epoch 23/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5015 - accuracy: 0.7920\n",
      "Epoch 24/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5013 - accuracy: 0.7922\n",
      "Epoch 25/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5012 - accuracy: 0.7923\n",
      "Epoch 26/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5033 - accuracy: 0.7911\n",
      "Epoch 27/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5033 - accuracy: 0.7905\n",
      "Epoch 28/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5037 - accuracy: 0.7912\n",
      "Epoch 29/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5059 - accuracy: 0.7882\n",
      "Epoch 30/30\n",
      "3392/3392 [==============================] - 5s 1ms/step - loss: 0.5052 - accuracy: 0.7885\n",
      "354/354 [==============================] - 0s 951us/step - loss: 0.5043 - accuracy: 0.7881\n",
      "accuracy: 78.81%\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=105, activation='relu', kernel_initializer=\"uniform\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(30, activation='relu', kernel_constraint=MaxNorm(3), kernel_initializer=\"uniform\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer=\"uniform\"))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_data, train_label, epochs=30, batch_size=10)\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(test_data, test_label)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Model Development\n",
    "### 4.2.1. Decision Tree\n",
    "\n",
    "For the decision tree classifier, I experimented with the splitting criteria, minimum samples required to split, max depth of the tree, minimum samples required at the leaf level and the maximum features to consider when looking for the best split. The following values of the parameters attained the best accuracy during classification. Results in the table below.\n",
    "\n",
    "*\t**Splitting criteria:** Gini Index (Using Gini Index marginally outperformed Entropy with a higher accuracy.)\n",
    "*\t**Min samples required to split:** 5% (Best amongst 1%, 10% and 5%.)\n",
    "*\t**Max Depth:** None\n",
    "*\t**Min samples required at leaf:**  0.1 % (Best amongst 1%, 5% and 0.1%.)\n",
    "*\t**Max features:** number of features (Performs better than 'auto', 'log2' and 'sqrt'.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desicion Tree using Gini Index : 85.14 percent.\n",
      "Desicion Tree using Entropy : 85.27 percent.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_measure</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DTree_Entropy</th>\n",
       "      <td>0.8527</td>\n",
       "      <td>0.7654</td>\n",
       "      <td>0.5771</td>\n",
       "      <td>0.6580</td>\n",
       "      <td>0.5771</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>0.1473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTree_Gini</th>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.6564</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.1486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  precision  recall  f_measure  sensitivity  \\\n",
       "DTree_Entropy    0.8527     0.7654  0.5771     0.6580       0.5771   \n",
       "DTree_Gini       0.8514     0.7592  0.5782     0.6564       0.5782   \n",
       "\n",
       "               specificity  error_rate  \n",
       "DTree_Entropy       0.9424      0.1473  \n",
       "DTree_Gini          0.9403      0.1486  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#printmd('### 3.1.1. Model Development ')\n",
    "\n",
    "# Gini \n",
    "clf_gini = tree.DecisionTreeClassifier(criterion = 'gini', min_samples_split = 0.05, min_samples_leaf = 0.001, max_features = None)\n",
    "clf_gini = clf_gini.fit(train_data, train_label)\n",
    "clf_gini_pred = clf_gini.predict(test_data)\n",
    "DTree_Gini = model_eval(test_label, clf_gini_pred)\n",
    "print('Desicion Tree using Gini Index : %.2f percent.' % (round(DTree_Gini['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "# Entropy\n",
    "clf_entropy = tree.DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 0.05, min_samples_leaf = 0.001)\n",
    "clf_entropy = clf_entropy.fit(train_data, train_label)\n",
    "clf_entropy_pred = clf_entropy.predict(test_data)\n",
    "DTree_Entropy = model_eval(test_label, clf_entropy_pred)\n",
    "print('Desicion Tree using Entropy : %.2f percent.' % (round(DTree_Entropy['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "#printmd('### 3.1.2. Model Evaulation ')\n",
    "ovl_dtree = round(pd.DataFrame([DTree_Entropy, DTree_Gini], index = ['DTree_Entropy','DTree_Gini']),4)\n",
    "display(ovl_dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree\n",
    "\n",
    "# from sklearn.tree import export_graphviz\n",
    "# from IPython.display import SVG\n",
    "# from graphviz import Source\n",
    "\n",
    "\n",
    "\n",
    "# graph = Source( tree.export_graphviz(clf, out_file=None, feature_names=train_data.columns))\n",
    "\n",
    "# # Display Tree\n",
    "# SVG(graph.pipe(format='svg'))\n",
    "\n",
    "# # Save Tree as PNG\n",
    "# png_bytes = graph.pipe(format='png')\n",
    "# with open('dtree_pipe.png','wb') as f:\n",
    "#     f.write(png_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Artificial Neural Network\n",
    "For the ANN classifier, I experimented with the activation function, the solver for weight optimization, regularization term and learning schedule for weight updates. The following values of the parameters attained the best accuracy during classification. Other parameters were neither applicable to the 'adam' solver nor did it improve the performance of the model. Results in the table below.\n",
    "\n",
    "*\t**Activation:** Logistic (Marginally outperformed 'relu', 'tanh' and 'identity' functions.)\n",
    "*   **Solver:** Adam (Works well on relatively large datasets with thousands of training samples or more)\n",
    "*   **Alpha:** 1e-4 (Best amongst 1, 1e-1, 1e-2, 1e-3, 1e-4 and 1e-5)\n",
    "*   **Learning Rate:**  'invscaling' (Gradually decreases the learning rate at each time step 't' using an inverse scaling exponent of 'power_t'.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN using TanH and lbfgs solver : 84.82 percent.\n",
      "ANN using relu and adam solver : 85.13 percent.\n",
      "ANN using logistic and adam solver : 85.09 percent.\n",
      "ANN using identity and adam solver : 85.18 percent.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_measure</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANN_TanH</th>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7318</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.1518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN_relu</th>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.7375</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>0.9291</td>\n",
       "      <td>0.1487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN_log</th>\n",
       "      <td>0.8509</td>\n",
       "      <td>0.7370</td>\n",
       "      <td>0.6106</td>\n",
       "      <td>0.6678</td>\n",
       "      <td>0.6106</td>\n",
       "      <td>0.9291</td>\n",
       "      <td>0.1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN_identity</th>\n",
       "      <td>0.8518</td>\n",
       "      <td>0.7468</td>\n",
       "      <td>0.6001</td>\n",
       "      <td>0.6655</td>\n",
       "      <td>0.6001</td>\n",
       "      <td>0.9338</td>\n",
       "      <td>0.1482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              accuracy  precision  recall  f_measure  sensitivity  \\\n",
       "ANN_TanH        0.8482     0.7318  0.6027     0.6610       0.6027   \n",
       "ANN_relu        0.8513     0.7375  0.6124     0.6692       0.6124   \n",
       "ANN_log         0.8509     0.7370  0.6106     0.6678       0.6106   \n",
       "ANN_identity    0.8518     0.7468  0.6001     0.6655       0.6001   \n",
       "\n",
       "              specificity  error_rate  \n",
       "ANN_TanH           0.9281      0.1518  \n",
       "ANN_relu           0.9291      0.1487  \n",
       "ANN_log            0.9291      0.1491  \n",
       "ANN_identity       0.9338      0.1482  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tan H\n",
    "ann_tanh = MLPClassifier(activation = 'tanh', solver='lbfgs', alpha=1e-1, hidden_layer_sizes=(10, 2), random_state=1, warm_start=True)\n",
    "ann_tanh.fit(train_data, train_label)                         \n",
    "ann_tanh_pred = ann_tanh.predict(test_data)\n",
    "ANN_TanH = model_eval(test_label, ann_tanh_pred)\n",
    "print('ANN using TanH and lbfgs solver : %.2f percent.' % (round(ANN_TanH['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "# Relu\n",
    "ann_relu = MLPClassifier(activation = 'relu', solver='adam', alpha=1e-1, \n",
    "                    hidden_layer_sizes=(5, 2), random_state=1,\n",
    "                    learning_rate  = 'invscaling',\n",
    "                    warm_start = True)\n",
    "ann_relu.fit(train_data, train_label)                         \n",
    "ann_relu_pred = ann_relu.predict(test_data)\n",
    "ANN_relu = model_eval(test_label, ann_relu_pred)\n",
    "print('ANN using relu and adam solver : %.2f percent.' % (round(ANN_relu['accuracy']*100,2)))\n",
    "\n",
    "# Log\n",
    "ann_log = MLPClassifier(activation = 'logistic', solver='adam', \n",
    "                    alpha=1e-4, hidden_layer_sizes=(5, 2),\n",
    "                    learning_rate  = 'invscaling', \n",
    "                    random_state=1, warm_start = True)\n",
    "ann_log.fit(train_data, train_label)                         \n",
    "ann_log_pred = ann_log.predict(test_data)\n",
    "ANN_log = model_eval(test_label, ann_log_pred)\n",
    "print('ANN using logistic and adam solver : %.2f percent.' % (round(ANN_log['accuracy']*100,2)))\n",
    "\n",
    "# Identity\n",
    "ann_identity = MLPClassifier(activation = 'identity', solver='adam', alpha=1e-1, hidden_layer_sizes=(5, 2), random_state=1, warm_start = True)\n",
    "ann_identity.fit(train_data, train_label)                         \n",
    "ann_identity_pred = ann_identity.predict(test_data)\n",
    "ANN_identity = model_eval(test_label, ann_identity_pred)\n",
    "print('ANN using identity and adam solver : %.2f percent.' % (round(ANN_identity['accuracy']*100,2)))\n",
    "\n",
    "#printmd('### 3.2.2. Model Evaulation ')\n",
    "ovl_ann = round(pd.DataFrame([ANN_TanH, ANN_relu, ANN_log, ANN_identity], index = ['ANN_TanH','ANN_relu', 'ANN_log', 'ANN_identity']),4)\n",
    "display(ovl_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Support Vector Machine\n",
    "For the SVM classifier, I experimented with the various available kernels, the penalty of the error term and the tolerance for stopping criteria. The following values of the parameters attained the best accuracy during classification. Results in the table below.\n",
    "\n",
    "*\t**Kernel:** rbf (Marginally outperformed 'linear, 'poly' and 'sigmoid' kernels.)\n",
    "*\t**C, penalty of the error term:** 1 (Best amongst 0.1, 0.5, 1 and 10)\n",
    "*\t**Tolerance for stopping criteria:** 1e-3 (Best amongst 1e-1, 1e-2, 1e-3, 1e-4 and 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using rbf kernel : 84.96 percent.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Linear kernel\u001b[39;00m\n\u001b[0;32m      9\u001b[0m svm_clf_linear \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43msvm_clf_linear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m svm_clf_linear_pred \u001b[38;5;241m=\u001b[39m svm_clf_linear\u001b[38;5;241m.\u001b[39mpredict(test_data)\n\u001b[0;32m     12\u001b[0m SVM_linear \u001b[38;5;241m=\u001b[39m model_eval(test_label, svm_clf_linear_pred)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\svm\\_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    319\u001b[0m (\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# rbf kernal\n",
    "svm_clf_rbf = svm.SVC(kernel = 'rbf', C = 1, tol = 1e-3)\n",
    "svm_clf_rbf.fit(train_data, train_label)\n",
    "svm_clf_rbf_pred = svm_clf_rbf.predict(test_data)\n",
    "SVM_rbf = model_eval(test_label, svm_clf_rbf_pred)\n",
    "print('SVM using rbf kernel : %.2f percent.' % (round(SVM_rbf['accuracy']*100,2)))\n",
    "\n",
    "# Linear kernel\n",
    "svm_clf_linear = svm.SVC(kernel = 'linear')\n",
    "svm_clf_linear.fit(train_data, train_label)\n",
    "svm_clf_linear_pred = svm_clf_linear.predict(test_data)\n",
    "SVM_linear = model_eval(test_label, svm_clf_linear_pred)\n",
    "print('SVM using linear kernel : %.2f percent.' % (round(SVM_linear['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "# Poly kernal\n",
    "svm_clf_poly = svm.SVC(kernel = 'poly')\n",
    "svm_clf_poly.fit(train_data, train_label)\n",
    "svm_clf_poly_pred = svm_clf_poly.predict(test_data)\n",
    "SVM_poly = model_eval(test_label, svm_clf_poly_pred)\n",
    "print('SVM using poly kernel : %.2f percent.' % (round(SVM_poly['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "svm_clf_sigmoid = svm.SVC(kernel = 'sigmoid')\n",
    "svm_clf_sigmoid.fit(train_data, train_label)\n",
    "svm_clf_sigmoid_pred = svm_clf_sigmoid.predict(test_data)\n",
    "SVM_sigmoid = model_eval(test_label, svm_clf_sigmoid_pred)\n",
    "print('SVM using sigmoid kernel : %.2f percent.' % (round(SVM_sigmoid['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "\n",
    "#printmd('### 3.3.2. Model Evaulation ')\n",
    "ovl_svm = round(pd.DataFrame([SVM_rbf, SVM_linear, SVM_poly, SVM_sigmoid], index = ['SVM_rbf','SVM_linear', 'SVM_poly', 'SVM_sigmoid']),4)\n",
    "display(ovl_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Ensemble Models\n",
    "\n",
    "### 4.2.4.1. Random Forest\n",
    "For the random forests classifier, I experimented with the number of trees, splitting criteria, minimum samples required to split, max depth of the tree, minimum samples required at the leaf level and the maximum features to consider when looking for the best split. The following values of the parameters attained the best accuracy during classification. Results in the table below.\n",
    "\n",
    "*\t**Num estimators:** 100 (Best amongst 10, 50 and 100)\n",
    "*\t**Splitting criteria:** Gini Index (Using Gini Index marginally outperformed Entropy with a higher accuracy.)\n",
    "*\t**Min samples required to split:** 5% (Best amongst 1%, 10% and 5%.)\n",
    "*\t**Max Depth:** None\n",
    "*\t**Min samples required at leaf:**  0.1 % (Best amongst 1%, 5% and 0.1%.)\n",
    "*\t**Max features:** number of features (Performs better than 'auto', 'log2' and 'sqrt'.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini\n",
    "r_forest_gini = RandomForestClassifier(n_estimators=100, criterion = 'gini', max_features = None,  min_samples_split = 0.05, min_samples_leaf = 0.001)\n",
    "r_forest_gini.fit(train_data, train_label)\n",
    "r_forest_gini_pred = r_forest_gini.predict(test_data)\n",
    "rforest_gini = model_eval(test_label, r_forest_gini_pred)\n",
    "print('Random Forest using Gini Index : %.2f percent.' % (round(rforest_gini['accuracy']*100,2)))\n",
    "\n",
    "# Entropy\n",
    "r_forest_entropy = RandomForestClassifier(n_estimators=100, criterion = 'entropy', max_features = None,  min_samples_split = 0.05, min_samples_leaf = 0.001)\n",
    "r_forest_entropy.fit(train_data, train_label)\n",
    "r_forest_entropy_pred = r_forest_entropy.predict(test_data)\n",
    "rforest_entropy = model_eval(test_label, r_forest_entropy_pred)\n",
    "print('Random Forest using Entropy : %.2f percent.' % (round(rforest_entropy['accuracy']*100,2)))\n",
    "\n",
    "#printmd('### 3.4.1.2. Model Evaulation ')\n",
    "ovl_rf = round(pd.DataFrame([rforest_gini, rforest_entropy], index = ['rforest_gini','rforest_entropy']),4)\n",
    "display(ovl_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4.2. Adaboost\n",
    "For the adaboost classifier, I experimented with base estimator from which the boosted ensemble is built and number of estimators. The following values of the parameters attained the best accuracy during classification. Results in the table below.\n",
    "\n",
    "*\t**Base Estimator:** DecisionTreeClassifier\n",
    "*\t**Num estimators:** 100 (Best amongst 10, 50 and 100.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=100)                     \n",
    "ada.fit(train_data, train_label)\n",
    "ada_pred = ada.predict(test_data)\n",
    "adaboost = model_eval(test_label, ada_pred)\n",
    "print('Adaboost : %.2f percent.' % (round(adaboost['accuracy']*100,2)))\n",
    "\n",
    "#printmd('### 3.4.2.2. Model Evaulation ')\n",
    "ovl_ada = round(pd.DataFrame([adaboost], index = ['adaboost']),4)\n",
    "display(ovl_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty = 'l2', dual = False, tol = 1e-4, fit_intercept = True, \n",
    "                            solver = 'liblinear')\n",
    "log_reg.fit(train_data, train_label)\n",
    "log_reg_pred = log_reg.predict(test_data)\n",
    "logistic_reg = model_eval(test_label, log_reg_pred)\n",
    "print('Logistic Regression : %.2f percent.' % (round(logistic_reg['accuracy']*100,3)))\n",
    "\n",
    "#printmd('### 3.5.2. Model Evaulation ')\n",
    "ovl_logreg = round(pd.DataFrame([logistic_reg], index = ['logistic_reg']),4)\n",
    "display(ovl_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6. k Nearest Neighbours\n",
    "For the K nearest neighbours classifier, I experimented with the num of neighbours values, every odd number ranging from 1 to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_outs = []\n",
    "for i in range(1,50,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(train_data, train_label) \n",
    "    knn_pred = knn.predict(test_data)\n",
    "    knn_perf = model_eval(test_label, knn_pred)\n",
    "    knn_perf['k'] = i\n",
    "    knn_outs.append(knn_perf)\n",
    "\n",
    "ovl_knn = round(pd.DataFrame(knn_outs),4)\n",
    "display(ovl_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Model Evaluation\n",
    "## 5.1. Overall Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_eval = pd.concat([ovl_dtree, ovl_ann, ovl_svm, ovl_rf, ovl_ada, ovl_logreg], axis = 0)\n",
    "overall_eval.sort_values(by = ['f_measure', 'accuracy'], ascending = False, inplace = True)\n",
    "\n",
    "printmd('Combing the performance statistics of all the model developed, as seen in table below, \\\n",
    "        we see that the ensemble model Adaboost hast the highest F-measure (0.6833), precision (0.7812) \\\n",
    "        and accuracy (0.8647). The Artificial neural network models are only marginally being in terms of \\\n",
    "        accuracy and F-measure. Almost all the model have an accuracy greater than 0.84, expect for two SVM \\\n",
    "        models. The table below lists the accuracy, error rate, F-measure, precision, recall, sensitivity and \\\n",
    "        specificity of all the models developed.')\n",
    "\n",
    "display(overall_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRoc(test_data, test_label, classifiers, pred_labels, plot_labels, limiter):\n",
    "    \n",
    "    color = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "    \n",
    "    y_test = label_binarize(test_label, classes=['<=50K', '>50K'])\n",
    "    plt.figure()\n",
    "    \n",
    "    for i in range(len(classifiers)):\n",
    "        \n",
    "        if plot_labels[i] not in limiter:\n",
    "            continue\n",
    "        \n",
    "        y_score = classifiers[i].predict_proba(test_data)\n",
    "        pos_class_index = list(np.unique(pred_labels[i])).index('>50K')\n",
    "        \n",
    "        fpr, tpr, thres = metrics.roc_curve(y_test.ravel(),y_score[:,pos_class_index], pos_label=1)\n",
    "                               \n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color=color[i % len(color)],lw=lw, label=plot_labels[i])\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "classifier_list = [clf_gini\n",
    "                ,clf_entropy\n",
    "                ,ann_tanh\n",
    "                ,ann_relu\n",
    "                ,ann_log\n",
    "                ,ann_identity\n",
    "#                 ,svm_clf_rbf\n",
    "#                 ,svm_clf_linear\n",
    "#                 ,svm_clf_poly\n",
    "#                 ,svm_clf_sigmoid\n",
    "                ,r_forest_gini\n",
    "                ,r_forest_entropy\n",
    "                ,ada\n",
    "                ,log_reg\n",
    "                ] \n",
    "pred_list = [clf_gini_pred\n",
    "            ,clf_entropy_pred\n",
    "            ,ann_tanh_pred\n",
    "            ,ann_relu_pred\n",
    "            ,ann_log_pred\n",
    "            ,ann_identity_pred\n",
    "#             ,svm_clf_rbf_pred\n",
    "#             ,svm_clf_linear_pred\n",
    "#             ,svm_clf_poly_pred\n",
    "#             ,svm_clf_sigmoid_pred\n",
    "            ,r_forest_gini_pred\n",
    "            ,r_forest_entropy_pred\n",
    "            ,ada_pred\n",
    "            ,log_reg_pred\n",
    "            ]\n",
    "\n",
    "clf_labels = ['DTree Gini'\n",
    "            ,'DTree Entropy'\n",
    "            ,'ANN TanH'\n",
    "            ,'ANN relu'\n",
    "            ,'ANN Logistic'\n",
    "            ,'ANN Identity'\n",
    "#             ,svm_clf_rbf_pred\n",
    "#             ,svm_clf_linear_pred\n",
    "#             ,svm_clf_poly_pred\n",
    "#             ,svm_clf_sigmoid_pred\n",
    "            ,'RForest Gini'\n",
    "            ,'RForest Entropy'\n",
    "            ,'Adaboost'\n",
    "            ,'Logistic Regression'\n",
    "            ]\n",
    "\n",
    "limiter = ['Adaboost', 'ANN TanH', 'ANN relu', 'ANN Logistic', 'Logistic Regression']\n",
    "generateRoc(test_data, test_label, classifier_list, pred_list, clf_labels, limiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above of the receiver operating characteristic curve for the top 5 models; Adaboost, ANN with logistics activation function, ANN with relu activation function, ANN with tanH activation function and logistic regression model. I chose to plot only the top 5 models as the ROC curves of most of the models overlap and the it is not easy to interpret the curve. \n",
    "\n",
    "From figure, we can see that the ROC curve of the Adaboost model has the highest lift and is closest to the top left corner (TPR of 1 and FPR of 0) of the plot. The Adaboost model's curve clearly separates itself from the ROC curves of the other 4 models, which overlap with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose **Adaboost** model as my preferred my approach. The Adaboost model not only has the **highest accuracy**, but also has the **highest precision and F-measure** of all the models developed as a part of this analysis. The advantages of using Adaboost over other models is that they are very simple to implement. Since they are made up of weak individual learners, they are less susceptible to overfitting. However, Adaboost is sensitive to noisy data and outliers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
